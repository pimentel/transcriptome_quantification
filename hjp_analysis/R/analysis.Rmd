---
title: "Simulation analysis"
output:
  html_document:
    toc: true
---

# Introduction

- `kallisto` results are from version 0.42.1
- `salmon` results are from version 0.3.2

This is a (re-)analysis of [Simon Cockell's](https://github.com/sjcockell)
[simulation on his
blog](http://sjcockell.me/2015/05/18/alignment-free-transcriptome-quantification/) done by [Harold Pimentel](http://twitter.com/hjpimentel).

Note that [Rob Patro](http://robpatro.com/) has also done a (re-)analysis of
this data (link on Simon's Blog). There are some differences in the
correlation coefficients. These are likely attributed to differences in [Pandas
and R](http://stats.stackexchange.com/questions/130815/differences-in-spearman-coefficient-between-r-and-pandas).

# Preliminaries

Load packages

```{r}
library("dplyr")
library("sleuth")
```

Some constants

```{r}
# This is the default from 'polyester' (see ?polyester::simulate_experiment )
MEAN_FL <- 250.0
# small number for plotting log things
SMALL <- 1e-2
```

## Load kallisto results


```{r}
kal <- read_kallisto_h5("../../sim_test_kallisto/abundance.h5", FALSE)
```

## Load Salmon results

```{r}
all_salmon <- lapply(paste0("sim_test_salmon_", 1:10),
  function(fname)
  {
    read_salmon(file.path("..", "..", fname, "quant.sf"))
  })
```

Ensure that all samon are ordered correctly:

```{r}
all_salmon %>%
  lapply(function(x) all.equal(x$target_id, .[[1]]$target_id)) %>%
  unlist() %>%
  Reduce(function(x, y) x && y, .)
```

Take averages

```{r}
salmon_tpm <- lapply(all_salmon, function(x) x$tpm) %>%
  as.data.frame() %>%
  apply(1, mean)
salmon_counts <- lapply(all_salmon, function(x) x$est_counts) %>%
  as.data.frame() %>%
  apply(1, mean)
salmon_avg <- data.frame(target_id = all_salmon[[1]]$target_id,
  tpm = salmon_tpm, est_counts = salmon_counts)
```

## Load oracle

Get the simulated target names

```{r,cache=TRUE}
transcripts <- Biostrings::readDNAStringSet("../../data/select_transcripts.fa")
transcript_names <- names(transcripts)
transcript_ids <- unlist(lapply(transcript_names,
    function(x) { substr(x, 1, 15) }))
target_ids <- data.frame(target_id = transcript_ids, stringsAsFactors = FALSE)
```
Load the oracle data and munge into a form that `sleuth` likes:

```{r}
oracle <- read.table("../../data/quant_bias_corrected.sf", sep="\t",
  stringsAsFactors = FALSE)
colnames(oracle) <- c("target_id", "len", "salmon_tpm", "fpkm", "counts")
oracle <- oracle %>%
  select(-c(fpkm)) %>%
  mutate(eff_len = len - MEAN_FL) %>%
  mutate(counts = round(counts))
```

The number of targets differs from the FASTA file to the number quantified
against (thanks, Rob). Because of this issue, we'll do a left join instead of
an inner join.

```{r}
oracle <- left_join(target_ids, oracle, by = "target_id")
```

Thus, we have `NA` values for columns where it didn't match the oracle counts.

```{r}
head(oracle)
```

Let's just assign zeroes to the effective length and to the counts for the
transcripts that didn't appear in the dataset that was quantified against.

```{r}
oracle <- oracle %>%
  mutate(
    len = ifelse(is.na(len), 0, len),
    eff_len = ifelse(is.na(eff_len), 0, eff_len),
    counts = ifelse(is.na(counts), 0, counts)
    )
```

Sanity check to ensure we're not getting fragments from things that have
effective length <= 0:

```{r}
oracle %>%
  filter(counts > 0 & eff_len <= 0)
```

Compute TPM from the rounded counts to get the _true_ distribution. Note, this
is going to be slightly different than the one read in.

```{r}
oracle <- oracle %>%
  mutate(tpm = counts_to_tpm(counts, eff_len),
    salmon_tpm = (salmon_tpm / sum(salmon_tpm)) * 1e6 ) %>%
  mutate(rel_diff = ifelse(tpm > 0, abs(tpm - salmon_tpm) / tpm, NA))
oracle$rel_diff %>%
  summary()
```

```{r}
oracle_salmon_tpm <- oracle %>%
  select(-c(tpm)) %>%
  rename(tpm = salmon_tpm)
```

```{r}
oracle <- oracle %>%
  select(-c(salmon_tpm, rel_diff))
```

Final sanity check:

```{r}
oracle %>%
  summary()
```

```{r}
all.equal(sort(kal$abundance$target_id), sort(oracle$target_id))
```

## Definition of error

The relative difference is defined as:

$$
d_i = \frac{x_i - y_i}{\frac{1}{2}\left| x_i + y_i\right|}
$$

where $y_i$ is the true value and $x_i$ is the estimate.

The relative error is similarly defined:

$$
e_i = \frac{x_i - y_i}{y_i}
$$

Note that the relative error is undefined when $y_i$ is zero.

```{r}
rel_diff <- function(x, y) {
  stopifnot(length(x) == length(y))

  result <- rep(NA_real_, length(x))

  non_zero <- which( x > 0 | y > 0 )
  both_zero <- setdiff(seq_along(x), non_zero)

  result[both_zero] <- 0.0

  result[non_zero] <- 2 * ((x[non_zero] - y[non_zero]) /
    abs(x[non_zero] + y[non_zero]))

  result
}
```

# Results

## Summarize the data using sleuth

```{r}
res <- merge_results(list(kal$abundance, salmon_avg), c("kallisto", "salmon_avg"), oracle)
```

`filtered_summary()` summarizes the results (comparing to oracle). When there is no
filter given, it uses everything. We used this function in the `kallisto`
preprint.

```{r}
res %>%
  filtered_summary() %>%
  lapply(print, width = 300) %>%
  invisible()
```

`med_rel_diff_no_zeroes` ignores when both the estimate and the truth are zero.
This is technically perfect, but if many transcripts have no reads mapping to
them, then it will skew the distribution causing the median to be artificially
low on the "interesting" transcripts (with expression > 0).

We can use filters similar to what Rob used in his notebook.

```{r}
res %>%
  filtered_summary(tpm_oracle >= 0.01 & est_counts_oracle > 1) %>%
  lapply(print, width = 300) %>%
  invisible()
```

## Figures

### TPM

Correlation

```{r,fig.width=14,fig.height=14}
ggplot(res$m_tpm, aes(log2(oracle + 1), log2(estimate + 1))) +
  geom_abline(alpha = 0.2, intercept = 0, slope = 1) +
  geom_point(alpha = 0.3) +
  theme_bw(20) +
  xlim(0, 17.5) +
  ylim(0, 17.5) +
  facet_wrap(~ method, ncol = 1)
```

#### Relative difference including zeroes


```{r}
rel_diff_tpm <- res$m_tpm %>%
  group_by(method) %>%
  mutate(relative_diff = rel_diff(oracle, estimate))
```

```{r,fig.width=14,fig.height=14}
ggplot(rel_diff_tpm, aes(oracle, relative_diff)) +
  geom_point(alpha = 0.2) +
  theme_bw(20) +
  scale_x_log10(limits = c(-1, 1e5)) +
  xlab("oracle TPM") +
  ylab("relative difference") +
  facet_wrap(~ method, ncol = 1)
```

```{r,fig.width=14,fig.height=14}
ggplot(rel_diff_tpm, aes(relative_diff)) +
  geom_histogram(binwidth = 0.02) +
  theme_bw(20) +
  xlab("relative difference") +
  facet_wrap(~ method, ncol = 1)
```

### Counts

Correlation

```{r,fig.width=14,fig.height=14}
ggplot(res$m_est_counts, aes(log2(oracle + 1), log2(estimate + 1))) +
  geom_abline(alpha = 0.2, intercept = 0, slope = 1) +
  geom_point(alpha = 0.3) +
  theme_bw(20) +
  xlim(0, 17.5) +
  ylim(0, 17.5) +
  facet_wrap(~ method, ncol = 1)
```

Relative difference

```{r}
rel_diff_est_counts <- res$m_est_counts %>%
  group_by(method) %>%
  mutate(relative_diff = rel_diff(oracle, estimate))
```

```{r,fig.width=14,fig.height=14}
ggplot(rel_diff_est_counts, aes(oracle, relative_diff)) +
  geom_point(alpha = 0.2) +
  theme_bw(20) +
  scale_x_log10(limits = c(-1, 1e5)) +
  xlab("oracle counts") +
  ylab("relative difference") +
  facet_wrap(~ method, ncol = 1)
```

```{r,fig.width=14,fig.height=14}
ggplot(rel_diff_est_counts, aes(relative_diff)) +
  geom_histogram(binwidth = 0.02) +
  theme_bw(20) +
  xlab("relative difference") +
  facet_wrap(~ method, ncol = 1)
```

# Appendix

## Functions from sleuth

```{r}
sleuth::merge_results
```

```{r}
sleuth::filtered_summary
```

```{r}
sleuth:::relative_difference
```

```{r}
sleuth:::percent_error
```

```{r}
sleuth::read_kallisto_h5
```

```{r}
sleuth::read_salmon
```

```{r}
sleuth::counts_to_tpm
```

## Session info

```{r}
sessionInfo()
```
